{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZIAkIlfmCe1B"
      },
      "source": [
        "# Simple Linear Regression With scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let’s start with the simplest case, which is simple linear regression.\n",
        "\n",
        "There are five basic steps when you’re implementing linear regression:\n",
        "\n",
        "1. Import the packages and classes you need.\n",
        "2. Provide data to work with and eventually do appropriate transformations.\n",
        "3. Create a regression model and fit it with existing data.\n",
        "4. Check the results of model fitting to know whether the model is satisfactory.\n",
        "5. Apply the model for predictions.\n",
        "\n",
        "These steps are more or less general for most of the regression approaches and implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DzbtdRcZDO9B"
      },
      "source": [
        "## Step 1: Imports\n",
        "\n",
        "The first step is to import the package numpy and the class LinearRegression from sklearn.linear_model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you have all the functionalities you need to implement linear regression.\n",
        "\n",
        "The fundamental data type of NumPy is the array type called numpy.ndarray. The rest of this article uses the term array to refer to instances of the type numpy.ndarray.\n",
        "\n",
        "The class sklearn.linear_model.LinearRegression will be used to perform linear and polynomial regression and make predictions accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Provide data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Set Information:**\n",
        "We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. It can also be used as a multi-class classification problem if the response is rounded to the nearest integer.\n",
        "\n",
        "**Attribute Information:**\n",
        "The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The aim is to use the eight features to predict each of the two responses.\n",
        "\n",
        "**Specifically:**\n",
        "- X1 Relative Compactness\n",
        "- X2 Surface Area\n",
        "- X3 Wall Area\n",
        "- X4 Roof Area\n",
        "- X5 Overall Height\n",
        "- X6 Orientation\n",
        "- X7 Glazing Area\n",
        "- X8 Glazing Area Distribution\n",
        "- y1 Heating Load\n",
        "- y2 Cooling Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "mydata = pd.read_excel(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'Y1', 'Y2'], dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mydata.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'y')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(mydata.X1, mydata.Y1)\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.array(mydata.X1).reshape((-1, 1))\n",
        "y = np.array(mydata.Y1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you have two arrays: the input x and output y. You should call .reshape() on x because this array is required to be two-dimensional, or to be more precise, to have one column and as many rows as necessary. That’s exactly what the argument (-1, 1) of .reshape() specifies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create a model and fit it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This statement creates the variable model as the instance of LinearRegression. You can provide several optional parameters to LinearRegression:\n",
        "\n",
        "- fit_intercept is a Boolean (True by default) that decides whether to calculate the intercept 𝑏₀ (True) or consider it equal to zero (False).\n",
        "- normalize is a Boolean (False by default) that decides whether to normalize the input variables (True) or not (False).\n",
        "- copy_X is a Boolean (True by default) that decides whether to copy (True) or overwrite the input variables (False).\n",
        "- n_jobs is an integer or None (default) and represents the number of jobs used in parallel computation. None usually means one job and -1 to use all processors.\n",
        "This example uses the default values of all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LinearRegression()\n",
        "model.fit(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With .fit(), you calculate the optimal values of the weights 𝑏₀ and 𝑏₁, using the existing input and output (x and y) as the arguments. In other words, .fit() fits the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Get results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coefficient of determination: 0.3872223619321592\n",
            "intercept: -23.053014060655016\n",
            "slope: [59.35905261]\n"
          ]
        }
      ],
      "source": [
        "r_sq = model.score(x, y)\n",
        "print('coefficient of determination:', r_sq)\n",
        "print('intercept:', model.intercept_)\n",
        "print('slope:', model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you’re applying .score(), the arguments are also the predictor x and regressor y, and the return value is 𝑅².\n",
        "The code above illustrates how to get 𝑏₀ and 𝑏₁. You can notice that .intercept_ is a scalar, while .coef_ is an array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Predict response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted response:\n",
            "[24.43422803 30.37013329 36.30603855]\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict([[0.8],[0.9],[1.0]])\n",
        "print('predicted response:', y_pred, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linier Regression With Keras Tansorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "X9uIpOS2zx7k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wwJGmDrQ0EoB"
      },
      "source": [
        "## Step 1: Define and Compile the Neural Network\n",
        "\n",
        "Next we will create the simplest possible neural network. It has 1 layer, and that layer has 1 neuron, and the input shape to it is just 1 value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "kQFAr_xo0M4T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x188225a2850>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2 = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "model2.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "model2.fit(x, y, epochs=2000, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Obtain Parameters/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[59.273663]], dtype=float32)>,\n",
              " <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([-23.040773], dtype=float32)>]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Predict response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oxNzL4lS2Gui"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[24.378157]\n",
            " [30.305521]\n",
            " [36.232887]]\n"
          ]
        }
      ],
      "source": [
        "print(model2.predict([[0.8],[0.9],[1.0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Linear Regression With statsmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can implement linear regression in Python relatively easily by using the package statsmodels as well. Typically, this is desirable when there is a need for more detailed results.\n",
        "\n",
        "The procedure is similar to that of scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import library\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# add column of one\n",
        "xwithone = sm.add_constant(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You need to add the column of ones to the inputs if you want statsmodels to calculate the intercept 𝑏₀. It doesn’t takes 𝑏₀ into account by default. That’s how you add the column of ones to x with add_constant(). It takes the input array x as an argument and returns a new array with the column of ones inserted at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a model and fit it\n",
        "model3 = sm.OLS(y, xwithone)\n",
        "results = model3.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should be careful here! Please, notice that the first argument is the output, followed with the input. There are several more optional parameters.\n",
        "\n",
        "By calling .fit(), you obtain the variable results, which is an instance of the class statsmodels.regression.linear_model.RegressionResultsWrapper. This object holds a lot of information about the regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.387\n",
            "Model:                            OLS   Adj. R-squared:                  0.386\n",
            "Method:                 Least Squares   F-statistic:                     484.0\n",
            "Date:                Sun, 24 Oct 2021   Prob (F-statistic):           1.59e-83\n",
            "Time:                        22:11:51   Log-Likelihood:                -2676.5\n",
            "No. Observations:                 768   AIC:                             5357.\n",
            "Df Residuals:                     766   BIC:                             5366.\n",
            "Df Model:                           1                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const        -23.0530      2.081    -11.076      0.000     -27.139     -18.967\n",
            "x1            59.3591      2.698     22.001      0.000      54.063      64.655\n",
            "==============================================================================\n",
            "Omnibus:                       53.989   Durbin-Watson:                   0.305\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               64.326\n",
            "Skew:                           0.708   Prob(JB):                     1.08e-14\n",
            "Kurtosis:                       3.059   Cond. No.                         15.0\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ],
      "source": [
        "# Get results\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Linear Regression With statmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.916\n",
            "Model:                            OLS   Adj. R-squared:                  0.915\n",
            "Method:                 Least Squares   F-statistic:                     1187.\n",
            "Date:                Sun, 24 Oct 2021   Prob (F-statistic):               0.00\n",
            "Time:                        22:11:51   Log-Likelihood:                -1912.5\n",
            "No. Observations:                 768   AIC:                             3841.\n",
            "Df Residuals:                     760   BIC:                             3878.\n",
            "Df Model:                           7                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         84.0134     19.034      4.414      0.000      46.649     121.378\n",
            "x1           -64.7734     10.289     -6.295      0.000     -84.973     -44.574\n",
            "x2            -0.0626      0.013     -4.670      0.000      -0.089      -0.036\n",
            "x3             0.0361      0.004      9.386      0.000       0.029       0.044\n",
            "x4            -0.0494      0.008     -6.569      0.000      -0.064      -0.035\n",
            "x5             4.1700      0.338     12.338      0.000       3.506       4.833\n",
            "x6            -0.0233      0.095     -0.246      0.805      -0.209       0.163\n",
            "x7            19.9327      0.814     24.488      0.000      18.335      21.531\n",
            "x8             0.2038      0.070      2.915      0.004       0.067       0.341\n",
            "==============================================================================\n",
            "Omnibus:                       18.647   Durbin-Watson:                   0.654\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               37.707\n",
            "Skew:                           0.044   Prob(JB):                     6.49e-09\n",
            "Kurtosis:                       4.082   Cond. No.                     3.35e+15\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 4.04e-23. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        }
      ],
      "source": [
        "# define new data\n",
        "X = mydata[[\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\"]].to_numpy()\n",
        "Xwithone = sm.add_constant(X)\n",
        "\n",
        "# Create a model and fit it\n",
        "model4 = sm.OLS(y, Xwithone)\n",
        "results = model4.fit()\n",
        "\n",
        "# Get results\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Multicollinearity with VIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import library\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multicollinearity occurs when there are two or more independent variables in a multiple regression model, which have a high correlation among themselves. When some features are highly correlated, we might have difficulty in distinguishing between their individual effects on the dependent variable. Multicollinearity can be detected using various techniques, one such technique being the Variance Inflation Factor (VIF)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  feature       VIF\n",
            "0      X4  5.245242\n",
            "1      X5  4.544944\n",
            "2      X7  4.284222\n",
            "3      X8  4.485473\n"
          ]
        }
      ],
      "source": [
        "# VIF dataframe\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = mydata.drop([\"X1\",\"X2\",\"X3\",\"X6\",\"Y1\",\"Y2\"], axis=1).columns\n",
        "  \n",
        "# calculating VIF for each feature\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(mydata.drop([\"X1\",\"X2\",\"X3\",\"X6\",\"Y1\",\"Y2\"], axis=1).values, i)\n",
        "                          for i in range(len(mydata.drop([\"X1\",\"X2\",\"X3\",\"X6\",\"Y1\",\"Y2\"], axis=1).columns))]\n",
        "                          \n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Linear Regression Model After Droping Multicollinearity Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we drop variables of X1,X2,X3, and X6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.865\n",
            "Model:                            OLS   Adj. R-squared:                  0.864\n",
            "Method:                 Least Squares   F-statistic:                     1222.\n",
            "Date:                Sun, 24 Oct 2021   Prob (F-statistic):               0.00\n",
            "Time:                        22:11:51   Log-Likelihood:                -2095.5\n",
            "No. Observations:                 768   AIC:                             4201.\n",
            "Df Residuals:                     763   BIC:                             4224.\n",
            "Df Model:                           4                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const        -13.8501      3.969     -3.490      0.001     -21.641      -6.059\n",
            "x1             0.0130      0.013      1.018      0.309      -0.012       0.038\n",
            "x2             5.4509      0.329     16.562      0.000       4.805       6.097\n",
            "x3            19.9327      1.031     19.333      0.000      17.909      21.957\n",
            "x4             0.2038      0.089      2.301      0.022       0.030       0.378\n",
            "==============================================================================\n",
            "Omnibus:                        9.852   Durbin-Watson:                   0.458\n",
            "Prob(Omnibus):                  0.007   Jarque-Bera (JB):               10.106\n",
            "Skew:                           0.278   Prob(JB):                      0.00639\n",
            "Kurtosis:                       2.915   Cond. No.                     5.42e+03\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 5.42e+03. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n"
          ]
        }
      ],
      "source": [
        "# define new data\n",
        "Xclean = mydata[[\"X4\",\"X5\",\"X7\",\"X8\"]].to_numpy()\n",
        "Xclean = sm.add_constant(Xclean)\n",
        "\n",
        "# Create a model and fit it\n",
        "model4 = sm.OLS(y, Xclean)\n",
        "results = model4.fit()\n",
        "\n",
        "# Get results\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ---------- Continue ----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Principal Components (PC) Regression with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Principal Component Analysis (PCA) is one of famous techniqeus for dimension reduction, feature extraction, and data visualization. In general, PCA is defined by a transformation of a high dimensional vector space into a low dimensional space. Let's consider visualization of 10-dim data. It is barely possible to effectively show the shape of such high dimensional data distribution. PCA provides an efficient way to reduce the dimensionalty (i.e., from 10 to 2), so it is much easier to visualize the shape of data distribution. PCA is also useful in the modeling of robust classifier where considerably small number of high dimensional training data is provided. By reducing the dimensions of learning data sets, PCA provides an effective and efficient method for data description and classification.\n",
        "\n",
        "**https://www.projectrhea.org/rhea/index.php/PCA_Theory_Examples**\n",
        "\n",
        "**http://www.dsc.ufcg.edu.br/~hmg/disciplinas/posgraduacao/rn-copin-2014.3/material/SignalProcPCA.pdf**\n",
        "\n",
        "![](https://miro.medium.com/max/922/1*m843SG1jDfU9-DLx-y34LQ.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we’ll import the necessary packages to perform principal components regression (PCR) in Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, when the predictor variables are highly correlated then multicollinearity can become a problem. This can cause the coefficient estimates of the model to be unreliable and have high variance.\n",
        "\n",
        "One way to avoid this problem is to instead use principal components regression, which finds M linear combinations (known as “principal components”) of the original p predictors and then uses least squares to fit a linear regression model using the principal components as predictors.\n",
        "\n",
        "This tutorial provides a step-by-step example of how to perform principal components regression in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standardize the Data\n",
        "PCA is effected by scale so you need to scale the features in your data before applying PCA. Use StandardScaler to help you standardize the dataset’s features onto unit scale (mean = 0 and variance = 1) which is a requirement for the optimal performance of many machine learning algorithms. If you want to see the negative effect not scaling your data can have, scikit-learn has a section on the effects of not standardizing your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://clavelresearch.files.wordpress.com/2019/03/z-score-population.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 2.04177671, -1.78587489, -0.56195149, -1.47007664,  1.        ,\n",
              "        -1.34164079, -1.76044698, -1.81457514],\n",
              "       [ 2.04177671, -1.78587489, -0.56195149, -1.47007664,  1.        ,\n",
              "        -0.4472136 , -1.76044698, -1.81457514],\n",
              "       [ 2.04177671, -1.78587489, -0.56195149, -1.47007664,  1.        ,\n",
              "         0.4472136 , -1.76044698, -1.81457514],\n",
              "       [ 2.04177671, -1.78587489, -0.56195149, -1.47007664,  1.        ,\n",
              "         1.34164079, -1.76044698, -1.81457514],\n",
              "       [ 1.28497917, -1.22923856,  0.        , -1.19867787,  1.        ,\n",
              "        -1.34164079, -1.76044698, -1.81457514]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Standardizing the features\n",
        "X = mydata[[\"X1\",\"X2\",\"X3\",\"X4\",\"X5\",\"X6\",\"X7\",\"X8\"]].to_numpy()\n",
        "x_std = StandardScaler().fit_transform(X)\n",
        "x_std[0:5,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigen Values & Eigen Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "In linear algebra, an eigenvector or characteristic vector of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue, often denoted by lambda , is the factor by which the eigenvector is scaled.\n",
        "\n",
        "![](http://www.sharetechnote.com/image/EngMath_Matrix_Eigen_Eq_02.PNG)\n",
        "\n",
        "We need to make the covariance matrix when we are doing eigendecomposition. For those who are not familiar with eigendecomposition, you can check this out: Eigendecomposition and PCA (**https://www.youtube.com/watch?v=-1iULsGndG8**), this video explains the role of the covariance in eigendecomposition thoroughly.\n",
        "\n",
        "**https://www.mathsisfun.com/algebra/eigenvalue.html**\n",
        "\n",
        "**https://www.youtube.com/watch?v=FgakZw6K1QQ&t=38s**\n",
        "\n",
        "**https://www.youtube.com/watch?v=G4N8vJpf7hM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw Eigenvalues: \n",
            " [3.70776711e+00 1.24146781e+00 5.28226666e-02 4.46132201e-03\n",
            " 3.94815168e-15 1.21454566e+00 1.00130378e+00 7.88061902e-01]\n",
            "Percentage of Variance Explained by Each Component: \n",
            " [4.62867411e-01 1.54981414e-01 6.59423589e-03 5.56939125e-04\n",
            " 4.92876358e-16 1.51620528e-01 1.25000000e-01 9.83794724e-02]\n"
          ]
        }
      ],
      "source": [
        "# Construct the covariance matrix.\n",
        "cov_mat = np.cov(x_std.T)\n",
        "\n",
        "# From this covariance matrix, caluclate the Eigenvalues and the Eigenvectors\n",
        "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "# print the Eigenvalues\n",
        "print(\"Raw Eigenvalues: \\n\", eigen_vals)\n",
        "# the sum of the Eigenvalues \n",
        "print(\"Percentage of Variance Explained by Each Component: \\n\", eigen_vals/sum(eigen_vals))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sorting the eigenvalues by decreasing order to rank the eigenvectors and cumulate them to figure out how many PCs are we going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.46286741, 0.61784882, 0.76946935, 0.89446935, 0.99284882,\n",
              "       0.99944306, 1.        , 1.        ])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's look at the cumulative variance described by each component\n",
        "tot = sum(eigen_vals)\n",
        "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
        "cum_var_exp = np.cumsum(var_exp)\n",
        "cum_var_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# make a bar plot of the variance associated with each component\n",
        "plt.bar(range(1,9), var_exp, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(1,9), cum_var_exp, where='mid', label='Cumulative explained variance', color='red')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PCA using sklearn\n",
        "After learning the steps of PCA, we can let sklearn to do the tedious and elaborate work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sklearn var:\n",
            " [4.62867411e-01 1.54981414e-01 1.51620528e-01 1.25000000e-01\n",
            " 9.83794724e-02 6.59423589e-03 5.56939125e-04 2.06690401e-32]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "\n",
        "# fit_transform() is used to calculate the PCAs from training data\n",
        "X_pca = pca.fit_transform(x_std)\n",
        "\n",
        "# to get the fit statistics (variance explained per component)\n",
        "print(\"sklearn var:\\n\", pca.explained_variance_ratio_)\n",
        "\n",
        "# like we did above visualize the PCs \n",
        "# and the cumulative variance explained by each PC\n",
        "plt.bar(range(1,9), pca.explained_variance_ratio_, alpha=0.5, align='center')\n",
        "plt.step(range(1,9), np.cumsum(pca.explained_variance_ratio_), where='mid', color='red')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.49595, -0.50173,  0.03251, -0.50496,  0.49624,  0.     ,\n",
              "         0.     ,  0.     ],\n",
              "       [-0.24473,  0.23154,  0.89429, -0.20612,  0.21036,  0.     ,\n",
              "        -0.     ,  0.     ],\n",
              "       [-0.     , -0.     , -0.     , -0.     , -0.     , -0.     ,\n",
              "        -0.70711, -0.70711],\n",
              "       [ 0.     ,  0.     ,  0.     ,  0.     ,  0.     , -1.     ,\n",
              "         0.     , -0.     ],\n",
              "       [ 0.     , -0.     , -0.     ,  0.     ,  0.     , -0.     ,\n",
              "        -0.70711,  0.70711],\n",
              "       [ 0.49517, -0.06621,  0.29112, -0.20516, -0.78967,  0.     ,\n",
              "         0.     ,  0.     ],\n",
              "       [-0.67003, -0.50488, -0.08744, -0.4501 , -0.29311, -0.     ,\n",
              "         0.     ,  0.     ],\n",
              "       [-0.     ,  0.65982, -0.32679, -0.67664, -0.     , -0.     ,\n",
              "        -0.     ,  0.     ]])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function of each component\n",
        "np.around(pca.components_,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**PC1 = 0.49595 X1 -0.50173 X2 + 0.03251 X3 -0.50496 X4 + ..... + 0.000 X8**\n",
        "\n",
        "**PC2 = -0.24473 X1 + 0.23154 X2 + 0.89429 X3 -0.20612 X4 + ..... + 0.000 X8**\n",
        "\n",
        "**etc....**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 3.12895415e+00, -9.02373432e-01,  2.52792238e+00, ...,\n",
              "         4.77611838e-01, -4.86937592e-02,  1.06699595e-14],\n",
              "       [ 3.12895415e+00, -9.02373432e-01,  2.52792238e+00, ...,\n",
              "         4.77611838e-01, -4.86937592e-02, -4.81727843e-16],\n",
              "       [ 3.12895415e+00, -9.02373432e-01,  2.52792238e+00, ...,\n",
              "         4.77611838e-01, -4.86937592e-02, -2.77463239e-15],\n",
              "       ...,\n",
              "       [-2.40682619e+00,  1.28785724e+00, -1.87764103e+00, ...,\n",
              "         1.39126500e-01, -1.13649220e-01, -4.76509499e-17],\n",
              "       [-2.40682619e+00,  1.28785724e+00, -1.87764103e+00, ...,\n",
              "         1.39126500e-01, -1.13649220e-01, -2.95016815e-17],\n",
              "       [-2.40682619e+00,  1.28785724e+00, -1.87764103e+00, ...,\n",
              "         1.39126500e-01, -1.13649220e-01, -1.13524131e-17]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# New Features\n",
        "X_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  feature  VIF\n",
            "0     PC1  1.0\n",
            "1     PC2  1.0\n",
            "2     PC3  1.0\n",
            "3     PC4  1.0\n",
            "4     PC5  1.0\n",
            "5     PC6  1.0\n",
            "6     PC7  1.0\n",
            "7     PC8  1.0\n"
          ]
        }
      ],
      "source": [
        "# VIF dataframe\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = [\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\"]\n",
        "  \n",
        "# calculating VIF for each component\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_pca, i)\n",
        "                          for i in range(8)]\n",
        "                          \n",
        "print(vif_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimate Principal Component Regression with Scikit-learn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coefficient of determination: 0.9162021649454063\n",
            "intercept: 22.3071953125\n",
            "slope: [ 4.16693352e+00  3.80271374e+00 -2.09979943e+00  2.60839659e-02\n",
            " -1.65312880e+00 -7.87243710e+00  6.09650035e+00 -2.10625862e-28]\n"
          ]
        }
      ],
      "source": [
        "# PC Regression using Scikit-learn\n",
        "model = LinearRegression()\n",
        "model.fit(X_pca, y)\n",
        "\n",
        "# Summary\n",
        "r_sq = model.score(X_pca, y)\n",
        "print('coefficient of determination:', r_sq)\n",
        "print('intercept:', model.intercept_)\n",
        "print('slope:', model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Y = 22.3071953125 + 4.1669 PC1 + 3.8027 PC2 -2.0998 PC3 + .... + 0.000 PC8** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimate Principal Component Regression with Statmodels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   R-squared:                       0.916\n",
            "Model:                            OLS   Adj. R-squared:                  0.915\n",
            "Method:                 Least Squares   F-statistic:                     1187.\n",
            "Date:                Sun, 24 Oct 2021   Prob (F-statistic):               0.00\n",
            "Time:                        22:11:52   Log-Likelihood:                -1912.5\n",
            "No. Observations:                 768   AIC:                             3841.\n",
            "Df Residuals:                     760   BIC:                             3878.\n",
            "Df Model:                           7                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         22.3072      0.106    210.678      0.000      22.099      22.515\n",
            "x1             4.1669      0.055     75.729      0.000       4.059       4.275\n",
            "x2             3.8027      0.095     39.990      0.000       3.616       3.989\n",
            "x3            -2.0998      0.096    -21.841      0.000      -2.289      -1.911\n",
            "x4             0.0261      0.106      0.246      0.805      -0.182       0.234\n",
            "x5            -1.6531      0.119    -13.851      0.000      -1.887      -1.419\n",
            "x6            -7.8724      0.461    -17.077      0.000      -8.777      -6.967\n",
            "x7             6.0965      1.586      3.843      0.000       2.983       9.210\n",
            "x8          9.921e-16   6.99e-17     14.190      0.000    8.55e-16    1.13e-15\n",
            "==============================================================================\n",
            "Omnibus:                       18.647   Durbin-Watson:                   0.654\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               37.707\n",
            "Skew:                           0.044   Prob(JB):                     6.49e-09\n",
            "Kurtosis:                       4.082   Cond. No.                     4.73e+15\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The smallest eigenvalue is 1.27e-28. This might indicate that there are\n",
            "strong multicollinearity problems or that the design matrix is singular.\n"
          ]
        }
      ],
      "source": [
        "## Estimate Principal Component Regression\n",
        "Xwithone_pca = sm.add_constant(X_pca)\n",
        "\n",
        "# Create a model and fit it\n",
        "model4 = sm.OLS(y, Xwithone_pca)\n",
        "results = model4.fit()\n",
        "\n",
        "# Get results\n",
        "print(results.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LASSO (L1) Regression with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is Regularization?\n",
        "In a general manner, to make things regular or acceptable is what we mean by the term regularization. This is exactly why we use it for applied machine learning. In the domain of machine learning, regularization is the process which prevents overfitting by discouraging developers learning a more complex or flexible model, and finally, which regularizes or shrinks the coefficients towards zero. The basic idea is to penalize the complex models i.e. adding a complexity term in such a way that it tends to give a bigger loss for evaluating complex models.\n",
        "\n",
        "There are mainly two types of regularization techniques, namely **Lasso Regression (L1 Regularization)** and **Ridge Regression (L2 Regularization)** . The way they assign a penalty to β (coefficients) is what differentiates them from each other.\n",
        "\n",
        "Lasso Regression (L1 Regularization). This regularization technique performs L1 regularization. Unlike Ridge Regression, it modifies the RSS by adding the penalty (shrinkage quantity) equivalent to the sum of the absolute value of coefficients.\n",
        "\n",
        "Looking at the equation below, we can observe that similar to Ridge Regression, Lasso (**Least Absolute Shrinkage and Selection Operator**) also penalizes the absolute size of the regression coefficients. In addition to this, it is quite capable of reducing the variability and improving the accuracy of linear regression models.\n",
        "\n",
        "![av](https://excelrcom.b-cdn.net/assets/admin/ckfinder/userfiles/images/2020%20uploads/13/2.PNG)\n",
        "\n",
        "- If the number of predictors (p) is greater than the number of observations (n), Lasso will pick at most n predictors as non-zero, even if all predictors are relevant (or may be used in the test set). In such cases, Lasso sometimes really has to struggle with such types of data.\n",
        "- If there are two or more highly collinear variables, then LASSO regression select one of them which is not good for the interpretation of data.\n",
        "\n",
        "Lasso regression differs from ridge regression in a way that it uses absolute values within the penalty function, rather than that of squares. This leads to penalizing (or equivalently constraining the sum of the absolute values of the estimates) values which causes some of the parameter estimates to turn out exactly zero. The more penalty is applied, the more the estimates get shrunk towards absolute zero. This helps to variable selection out of given range of n variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import library\n",
        "from sklearn.linear_model import ElasticNet, Ridge, Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R Square: 0.9055917500306943\n",
            "Intercept: -25.671999350888107\n",
            "Slope: [-0.00000000e+00  4.01453926e-03  4.81692259e-02 -0.00000000e+00\n",
            "  4.92842241e+00 -0.00000000e+00  1.41309347e+01  2.68281344e-01]\n"
          ]
        }
      ],
      "source": [
        "# Define model\n",
        "lasso = Lasso(alpha=0.1)\n",
        "\n",
        "# Fit the Lasso model\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Calculate model score (R square)\n",
        "print(\"R Square:\", lasso.score(X, y))\n",
        "\n",
        "# Lasso Coeficients\n",
        "print(\"Intercept:\", lasso.intercept_)\n",
        "print(\"Slope:\", lasso.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimate LASSO Regression using optimum Alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Call function\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
        "from sklearn.model_selection import KFold, GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neg. RMSE : -3.096\n",
            "Config best {'alpha': 0.1}\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = Lasso(max_iter = 5000)\n",
        "# define model evaluation method\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# define grid\n",
        "grid = {'alpha': [0.1,0.2,0.3,0.4,0.5]}\n",
        "# define search\n",
        "search = GridSearchCV(model, grid, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n",
        "# perform the search\n",
        "results = search.fit(X, y)\n",
        "# summarize\n",
        "print('Neg. RMSE : %.3f' % results.best_score_)\n",
        "print('Config best %s' % results.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coefficient of determination: 0.9055917500306943\n",
            "intercept: -25.671999350888107\n",
            "slope:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([-0.0000e+00,  4.0000e-03,  4.8000e-02, -0.0000e+00,  4.9280e+00,\n",
              "       -0.0000e+00,  1.4131e+01,  2.6800e-01])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r_sq = results.best_estimator_.score(X, y)\n",
        "print('coefficient of determination:', r_sq)\n",
        "print('intercept:',results.best_estimator_.intercept_)\n",
        "print('slope:')\n",
        "np.round(results.best_estimator_.coef_,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using RidgeCV function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alpha: 0.1\n",
            "intercept: -25.671999350888107\n",
            "slope:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([-0.0000e+00,  4.0000e-03,  4.8000e-02, -0.0000e+00,  4.9280e+00,\n",
              "       -0.0000e+00,  1.4131e+01,  2.6800e-01])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define evaluation method of the model\n",
        "model_lasso = LassoCV(alphas = [0.1,0.2,0.3,0.4,0.5], cv=cv, max_iter = 3000).fit(X, y)\n",
        "model_lasso.score(X, y)\n",
        "print('Alpha:', model_lasso.alpha_)\n",
        "print('intercept:',model_lasso.intercept_)\n",
        "print('slope:')\n",
        "np.round(model_lasso.coef_,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ridge (L2) Regression with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ridge Regression (L2 Regularization). This technique performs L2 regularization. The main algorithm behind this is to modify the RSS by adding the penalty which is equivalent to the square of the magnitude of coefficients. However, it is considered to be a technique used when the info suffers from multicollinearity (independent variables are highly correlated). In multicollinearity, albeit the smallest amount squares estimates (OLS) are unbiased, their variances are large which deviates the observed value faraway from truth value. By adding a degree of bias to the regression estimates, ridge regression reduces the quality errors. It tends to solve the multicollinearity problem through shrinkage parameter λ. Now, let us have a look at the equation below.\n",
        "\n",
        "![av](https://excelrcom.b-cdn.net/assets/admin/ckfinder/userfiles/images/2020%20uploads/13/1.PNG)\n",
        "\n",
        "In this equation, we have two components. The foremost one denotes the least square term and later one is lambda of the summation of β2 (beta- square) where β is the coefficient. This is added to least square term so as to shrink the parameter to possess a really low variance.\n",
        "\n",
        "Every technique has some pros and cons, so as Ridge regression. It decreases the complexity of a model but does not reduce the number of variables since it never leads to a coefficient tending to zero rather only minimizes it. Hence, this model is not a good fit for feature reduction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neg. RMSE : -2.946\n",
            "Config best {'alpha': 0.1}\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = Ridge()\n",
        "# define model evaluation method\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# define grid\n",
        "grid = {'alpha': [0.1,0.2,0.3,0.4,0.5]}\n",
        "# define search\n",
        "search = GridSearchCV(model, grid, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n",
        "# perform the search\n",
        "results = search.fit(X, y)\n",
        "# summarize\n",
        "print('Neg. RMSE : %.3f' % results.best_score_)\n",
        "print('Config best %s' % results.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coefficient of determination: 0.9148735698191434\n",
            "intercept: 19.112558611659537\n",
            "slope:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([-2.91083e+01, -1.77000e-02,  3.46000e-02, -2.62000e-02,\n",
              "        4.91520e+00, -2.33000e-02,  1.97806e+01,  2.06500e-01])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r_sq = results.best_estimator_.score(X, y)\n",
        "print('coefficient of determination:', r_sq)\n",
        "print('intercept:',results.best_estimator_.intercept_)\n",
        "print('slope:')\n",
        "np.round(results.best_estimator_.coef_,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alpha: 0.1\n",
            "intercept: 19.112558611659537\n",
            "slope:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([-29.,  -0.,   0.,  -0.,   5.,  -0.,  20.,   0.])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define evaluation method of the model\n",
        "model_ridge = RidgeCV(alphas = [0.1,0.2,0.3,0.4,0.5], cv=cv, scoring=\"neg_root_mean_squared_error\").fit(X, y)\n",
        "model_ridge.score(X, y)\n",
        "print('Alpha:', model_ridge.alpha_)\n",
        "print('intercept:',model_ridge.intercept_)\n",
        "print('slope:')\n",
        "np.round(model_ridge.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elastic Net Regression with scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main purpose of ElasticNet Regression is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients. ElasticNet combines L1 and L2 (Lasso and Ridge) approaches. As a result, it performs a more efficient smoothing process. In another source, it is said that Elastic Net first emerged as a result of critique on Lasso, whose variable selection can be too dependent on data and thus unstable. The solution is to combine the penalties of Ridge regression and Lasso to get the best of both worlds.\n",
        "\n",
        "**Features of ElasticNet Regression:**\n",
        "- It combines the L1 and L2 approaches.\n",
        "- It performs a more efficient regularization process.\n",
        "- It has two parameters to be set, λ and α.\n",
        "\n",
        "The elastic net method improves on lasso’s limitations, i.e., where lasso takes a few samples for high dimensional data, the elastic net procedure provides the inclusion of “n” number of variables until saturation. In a case where the variables are highly correlated groups, lasso tends to choose one variable from such groups and ignore the rest entirely. Elastic Net aims at minimizing the following loss function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://miro.medium.com/max/589/1*XjDc54wcUkLcnSXmYjIH4Q.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let’s build a ElasticNet Regression model on a sample data set. And then let’s calculate the square root of the model’s Mean Squared Error This will give us the model error. First of all, we import the libraries necessary for modeling as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neg. RMSE : -3.672\n",
            "Config best {'alpha': 0.1, 'l1_ratio': 0.5}\n"
          ]
        }
      ],
      "source": [
        "# define model\n",
        "model = ElasticNet()\n",
        "# define model evaluation method\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "# define grid\n",
        "grid = {'alpha': [0.1,0.2,0.3,0.4,0.5],'l1_ratio': [0.1,0.2,0.3,0.4,0.5]}\n",
        "# define search\n",
        "search = GridSearchCV(model, grid, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n",
        "# perform the search\n",
        "results = search.fit(X, y)\n",
        "# summarize\n",
        "print('Neg. RMSE : %.3f' % results.best_score_)\n",
        "print('Config best %s' % results.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "coefficient of determination: 0.8674865824188637\n",
            "intercept: -13.0770631841744\n",
            "slope:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([-0.    , -0.0076,  0.0596, -0.0111,  4.0219, -0.    ,  4.3127,\n",
              "        0.4591])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r_sq = results.best_estimator_.score(X, y)\n",
        "print('coefficient of determination:', r_sq)\n",
        "print('intercept:',results.best_estimator_.intercept_)\n",
        "print('slope:')\n",
        "np.round(results.best_estimator_.coef_,4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colab1-for-deeplearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "431a35f87f30a76417cdd0c4f34651cdd199ab3210df0eaa8d33365a9e456766"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

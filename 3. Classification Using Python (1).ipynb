{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Supervised learning Algorithms can be further divided into two types of problems:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/supervised-machine-learning-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How Supervised Learning Works?\n",
        "\n",
        "In supervised learning, models are trained using labelled dataset, where the model learns about each type of data. Once the training process is completed, the model is tested on the basis of test data (a subset of the training set), and then it predicts the output.\n",
        "\n",
        "The working of Supervised learning can be easily understood by the below example and diagram:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/supervised-machine-learning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose we have a dataset of different types of shapes which includes square, rectangle, triangle, and Polygon. Now the first step is that we need to train the model for each shape.\n",
        "\n",
        "If the given shape has four sides, and all the sides are equal, then it will be labelled as a Square.\n",
        "If the given shape has three sides, then it will be labelled as a triangle.\n",
        "If the given shape has six equal sides then it will be labelled as hexagon.\n",
        "Now, after training, we test our model using the test set, and the task of the model is to identify the shape.\n",
        "\n",
        "The machine is already trained on all types of shapes, and when it finds a new shape, it classifies the shape on the bases of a number of sides, and predicts the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What Is Classification?\n",
        "Classification is the process of recognizing, understanding, and grouping ideas and objects into preset categories or “sub-populations.” Using pre-categorized training datasets, machine learning programs use a variety of algorithms to classify future datasets into categories.\n",
        "\n",
        "Classification algorithms in machine learning use input training data to predict the likelihood that subsequent data will fall into one of the predetermined categories. One of the most common uses of classification is filtering emails into “spam” or “non-spam.” \n",
        "\n",
        "In short, classification is a form of “pattern recognition,” with classification algorithms applied to the training data to find the same pattern (similar words or sentiments, number sequences, etc.) in future sets of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Popular Classification Algorithms:\n",
        "- Logistic Regression\n",
        "- K-Nearest Neighbors\n",
        "- Decision Tree\n",
        "- Support Vector Machines\n",
        "- Naive Bayes\n",
        "- Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZIAkIlfmCe1B"
      },
      "source": [
        "# 1. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic regression is a calculation used to predict a binary outcome: either something happens, or does not. This can be exhibited as Yes/No, Pass/Fail, Alive/Dead, etc. \n",
        "\n",
        "Independent variables are analyzed to determine the binary outcome with the results falling into one of two categories. The independent variables can be categorical or numeric, but the dependent variable is always categorical. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c6fbaf6182755b528d3232d60408a2414b6d76c1)\n",
        "\n",
        "or\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/da88d81e6a0169ec2da7d7bc0e0f4efef4b9e2c7)\n",
        "\n",
        "where usually b=exp()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://miro.medium.com/max/875/1*RqXFpiNGwdiKBWyLJc_E7g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Types of Logistic Regression\n",
        "1. Binary Logistic Regression:\n",
        "The categorical response has only two 2 possible outcomes. Example: Spam or Not\n",
        "2. Multinomial Logistic Regression:\n",
        "Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
        "3. Ordinal Logistic Regression:\n",
        "Three or more categories with ordering. Example: Movie rating from 1 to 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimation of coefficients\n",
        "The regression coefficients are usually estimated using maximum likelihood estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://raw.githubusercontent.com/rauzansumara/introduction-to-machine-learning/master/Material/image1.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unlike linear regression, it is not possible to find a closed-form expression for the coefficient values that maximize the likelihood function, so that an iterative process must be used instead; \n",
        "for example:\n",
        "- Newton's method \n",
        "- gradient descent \n",
        "\n",
        "This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until no more improvement is made, at which point the process is said to have converged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# f(β), ∇f(β), and ∇²f(β)\n",
        "\n",
        "f(β) = likelihood function\n",
        "\n",
        "∇f(β) = the first derivative (gradient of f(β))\n",
        "\n",
        "∇²f(β) = the second derivative (Hassian Matrix of f(β))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://raw.githubusercontent.com/rauzansumara/introduction-to-machine-learning/master/Material/image2.1.jpeg)\n",
        "![](https://raw.githubusercontent.com/rauzansumara/introduction-to-machine-learning/master/Material/image2.2.jpeg)\n",
        "![](https://raw.githubusercontent.com/rauzansumara/introduction-to-machine-learning/master/Material/image2.3.jpeg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Source:\n",
        "Owner of database: Volker Lohweg (University of Applied Sciences, Ostwestfalen-Lippe, volker.lohweg '@' hs-owl.de)\n",
        "\n",
        "Donor of database: Helene DÃ¶rksen (University of Applied Sciences, Ostwestfalen-Lippe, helene.doerksen '@' hs-owl.de)\n",
        "\n",
        "Date received: August, 2012\n",
        "\n",
        "## Data Set Information:\n",
        "Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n",
        "\n",
        "\n",
        "## Attribute Information:\n",
        "1. variance of Wavelet Transformed image (continuous) (X1)\n",
        "2. skewness of Wavelet Transformed image (continuous) (X2)\n",
        "3. curtosis of Wavelet Transformed image (continuous) (X3)\n",
        "4. entropy of image (continuous) (X4)\n",
        "5. class (integer) (Y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DzbtdRcZDO9B"
      },
      "source": [
        "## Imports\n",
        "\n",
        "The first step is to import the package numpy and the class LinearRegression from sklearn.linear_model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "mydata = pd.read_table(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\",\n",
        "        delimiter=\",\", header=0, names=[\"X1\",\"X2\",\"X3\",\"X4\",\"Y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1371"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mydata.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get input \"X1\",\"X2\",\"X3\",\"X4\"\n",
        "inputX = mydata[[\"X1\",\"X2\",\"X3\",\"X4\"]].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# define input X\n",
        "X = np.matrix(np.ones(mydata.shape[0])).T\n",
        "X = np.append(X,inputX,axis=1)\n",
        "\n",
        "# define output Y\n",
        "Y = mydata[[\"Y\"]].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression: Newton Raphson Approach\n",
        "\n",
        "he Newton-Raphson method (also known as Newton's method) is a way to quickly find a good approximation for the root of a real-valued function f(x)=0. It uses the idea that a continuous and differentiable function can be approximated by a straight line tangent to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://raw.githubusercontent.com/rauzansumara/introduction-to-machine-learning/master/Material/image2.5.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define f(β), ∇f(β), and ∇²f(β)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://raw.githubusercontent.com/rauzansumara/introduction-to-machine-learning/master/Material/image2.2.jpeg)\n",
        "![](https://raw.githubusercontent.com/rauzansumara/introduction-to-machine-learning/master/Material/image2.3.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# f(β)\n",
        "def f(beta):\n",
        "    return np.ravel(np.ones(len(Y))*(np.log(1+np.exp(X*beta)))-Y.T*X*beta)[0]\n",
        "\n",
        "# ∇f(β)\n",
        "def nabla_f(beta):\n",
        "    return X.T*(1/(1+1/np.exp(X*beta))-Y)\n",
        "\n",
        "# ∇²f(β)    \n",
        "def nabla2_f(beta):\n",
        "    return X.T*(np.diag(np.ravel(np.exp(X*beta)/np.power(1+np.exp(X*beta),2)))*X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter = 13\n",
            "[[ 7.32180471]\n",
            " [-7.85933049]\n",
            " [-4.19096321]\n",
            " [-5.28743068]\n",
            " [-0.60531897]]\n",
            "norm = 1.5942825815928695e-13\n"
          ]
        }
      ],
      "source": [
        "# Newton raphson Method\n",
        "beta = np.matrix(np.zeros(X.shape[1])).T\n",
        "TOL = np.power(10.,-10)\n",
        "counter = 0\n",
        "\n",
        "while np.linalg.norm(nabla_f(beta)) > TOL:\n",
        "  counter += 1\n",
        "  beta -= np.linalg.inv(nabla2_f(beta))*nabla_f(beta)\n",
        "  \n",
        "print('iter =',counter)\n",
        "print(beta)\n",
        "print('norm =',np.linalg.norm(nabla_f(beta)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression: Gradient Descent Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter = 164906\n",
            "[[ 7.32180471]\n",
            " [-7.85933049]\n",
            " [-4.19096321]\n",
            " [-5.28743068]\n",
            " [-0.60531897]]\n",
            "norm = 9.998916234054437e-11\n"
          ]
        }
      ],
      "source": [
        "# Gradient Descent\n",
        "beta = np.matrix(np.zeros(X.shape[1])).T\n",
        "TOL = np.power(10.,-10)\n",
        "lam = 0.001 # learning_rate\n",
        "counter = 0\n",
        "\n",
        "while np.linalg.norm(nabla_f(beta)) > TOL:\n",
        "  counter += 1\n",
        "  beta -= lam*nabla_f(beta)\n",
        "    \n",
        "print('iter =',counter)\n",
        "print(beta)\n",
        "print('norm =',np.linalg.norm(nabla_f(beta)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "matrix([[0.01609821]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Classification Process\n",
        "Xtest = [[1, 0.4,0.5,1.0,1.5]]\n",
        "p = (np.exp(np.dot(Xtest, beta)) / (1 + np.exp(np.dot(Xtest, beta))))\n",
        "p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression: Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 7.32180341 -7.85932911 -4.19096249 -5.28742975 -0.60531885]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Newton-Conjugate Gradient\n",
        "clf1 = LogisticRegression(penalty=\"none\",solver='newton-cg',fit_intercept=False)\n",
        "clf1.fit(X,np.ravel(Y))\n",
        "print(clf1.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 4.17766542 -4.16803244 -2.3129587  -2.84284465 -0.22558116]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rauzan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n"
          ]
        }
      ],
      "source": [
        "# Stochastic Average Gradient\n",
        "clf2 = LogisticRegression(penalty=\"none\",solver='sag',fit_intercept=False) \n",
        "clf2.fit(X,np.ravel(Y))\n",
        "print(clf2.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0], dtype=int64)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf2.predict([[1, 0.4,0.5,1.0,1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "K nearest neighbors is a simple algorithm used for classification. It basically stores all available cases to classify the new cases by a majority vote of its k neighbors. The case assigned to the class is most common amongst its K nearest neighbors measured by a distance function (Euclidean, Manhattan, Minkowski, and Hamming).\n",
        "\n",
        "While the three former distance functions are used for continuous variables, the Hamming distance function is used for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing K turns out to be a challenge while performing kNN modeling.\n",
        "\n",
        "## Distance metrics\n",
        "![](https://raw.githubusercontent.com/rauzansumara/introduction-to-machine-learning/master/Material/image3.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Propreties\n",
        "- K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
        "- K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n",
        "- K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
        "- K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.\n",
        "- It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n",
        "- KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.\n",
        "- Example: Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why do we need a K-NN Algorithm?\n",
        "\n",
        "Suppose there are two categories, i.e., Category A and Category B, and we have a new data point x1, so this data point will lie in which of these categories. To solve this type of problem, we need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset. Consider the below diagram:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How does K-NN work?\n",
        "\n",
        "The K-NN working can be explained on the basis of the below algorithm:\n",
        "\n",
        "- Step-1: Select the number K of the neighbors\n",
        "- Step-2: Calculate the Euclidean distance of K number of neighbors\n",
        "- Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n",
        "- Step-4: Among these k neighbors, count the number of the data points in each category.\n",
        "- Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n",
        "- Step-6: Our model is ready."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose we have a new data point and we need to put it in the required category. Consider the below image:\n",
        "\n",
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Firstly, we will choose the number of neighbors, so we will choose the k=5.\n",
        "- Next, we will calculate the Euclidean distance between the data points. The Euclidean distance is the distance between two points, which we have already studied in geometry. It can be calculated as:\n",
        "\n",
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- By calculating the Euclidean distance we got the nearest neighbors, as three nearest neighbors in category A and two nearest neighbors in category B. Consider the below image:\n",
        "\n",
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- As we can see the 3 nearest neighbors are from category A, hence this new data point must belong to category A."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Things to Consider Before Selecting KNN:\n",
        "- KNN is computationally expensive\n",
        "- Variables should be normalized else higher range variables can bias it\n",
        "- Works on pre-processing stage more before going for kNN like an outlier, noise removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "df = pd.read_table(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\",\n",
        "        delimiter=\",\", header=0, names=[\"X1\",\"X2\",\"X3\",\"X4\",\"Y\"])\n",
        "\n",
        "X = df[[\"X1\",\"X2\",\"X3\",\"X4\"]].to_numpy()\n",
        "Y = df[[\"Y\"]].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Scaling (MinMax Normalize) \n",
        "def NormalizeData(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "# Normalize dataset\n",
        "datX = NormalizeData(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate random values\n",
        "nr = np.arange(datX.shape[0])\n",
        "\n",
        "# shuffle datasets\n",
        "training_ratio = 0.8\n",
        "np.random.shuffle(nr)\n",
        "datX = datX[nr] # shuffle X\n",
        "datY = Y[nr] # shuffle Y\n",
        "\n",
        "# set 80% train and 20% test sets\n",
        "nr_split = np.round(datX.shape[0]*training_ratio, 0).astype(np.int)\n",
        "\n",
        "# Divide data into train and test sets\n",
        "X_train = datX[:nr_split, :]\n",
        "y_train = datY[:nr_split]\n",
        "X_test = datX[nr_split:, :]\n",
        "y_test = datY[nr_split:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# Create kNN function\n",
        "def knn_classifier(X_train, y_train, X_test, k):\n",
        "    A = np.expand_dims(X_test, axis=1) \n",
        "    B = np.expand_dims(X_train, axis = 0)\n",
        "    X_tensor = A - B\n",
        "\n",
        "    D = np.sqrt(np.sum(np.power(X_tensor, 2), axis=2))\n",
        "    nr = np.argsort(D, axis = 1)\n",
        "    return stats.mode(y_train[nr][:,:k], axis = 1)[0][:,0]\n",
        "\n",
        "# call k-NN function\n",
        "y_test_est = knn_classifier(X_train, y_train, X_test, k = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]], dtype=int64)"
            ]
          },
          "execution_count": 192,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# result\n",
        "y_test_est[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. K-Nearest Neighbors: Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Splitting the dataset into training and test set.  \n",
        "# from sklearn.model_selection import train_test_split  \n",
        "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.20, random_state=0)  \n",
        "\n",
        "# # Normalize Data\n",
        "# from sklearn.preprocessing import MinMaxScaler    \n",
        "# st_x = MinMaxScaler()    \n",
        "# x_train = st_x.fit_transform(X_train)    \n",
        "# x_test = st_x.transform(X_test)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=3)"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fitting K-NN classifier to the training set  \n",
        "from sklearn.neighbors import KNeighborsClassifier  \n",
        "classifier = KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=2)  \n",
        "classifier.fit(X_train, np.ravel(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predicting the test set  \n",
        "y_pred = classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 1, 1, 1, 0, 1, 0], dtype=int64)"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred[:10]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colab1-for-deeplearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "431a35f87f30a76417cdd0c4f34651cdd199ab3210df0eaa8d33365a9e456766"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
